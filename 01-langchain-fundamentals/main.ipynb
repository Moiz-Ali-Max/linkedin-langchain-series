{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611d6124",
   "metadata": {},
   "source": [
    "#### 1. Load Pdf Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64317483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4105391",
   "metadata": {},
   "source": [
    "##### Use GEMINI Api\n",
    "- Here we are using google gemini api key, which we set in .env files.\n",
    "- You can create this API Key from Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e224bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"GEMINI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc93c6f",
   "metadata": {},
   "source": [
    "##### Use GEMINI Model\n",
    "- Now you can use whatever model you want of google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6211f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebd3f6",
   "metadata": {},
   "source": [
    "##### Load Pdf Files\n",
    "- Here we are ready to extract any data of pdf files.\n",
    "- Because we set the langchain and use gemini model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d5f234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content=\"Day 1 - LangChain \\nFundamentals and Data Loader\\nIntroduction to LangChain for Beginners\\nWelcome to this beginner-friendly guide to LangChain! We'll explore the basics, \\nsetup, and how to work with different data loaders.\\nWhat is LangChain?\\nLangChain is a framework for building applications with large language models \\n\\ue081LLMs). Think of it as a toolkit that helps you connect AI models to different \\ndata sources and applications.\\nLangChain is like a bridge between AI models and your data. It helps you:\\nTalk to AI models\\nFeed your own data to these models\\nBuild useful applications (like chatbots that know about your documents)\\nSetting Up LangChain\\nLet's start with how to set up LangChain on your machine:\\n# Install LangChain and OpenAI packages\\npip install langchain\\npip install google-generativeai\\npip install pypdf  \\npip install bs4    \\npip install unstructured \\npip install request\\npip install python-docx2txt\\npip install unstructured[pptx]\\npip install jq\\npip install markdown\\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n1\"), Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content=\"#Another Libraries\\npip install python-dotenv\\nThat's it! Now you're ready to start using LangChain.\\nCore Concepts in LangChain\\nBefore diving into data loaders, let's understand the basic building blocks:\\nComponent What it does Real-world example\\nLLMs AI models that generate text ChatGPT, Llama, etc.\\nPrompts Templates for\\ncommunicating with LLMs\\nAsking a question in a specific\\nformat\\nChains Link multiple components\\ntogether\\nReading data, summarizing it, then\\nanswering questions\\nDocument\\nLoaders Tools to bring in outside data Reading PDFs, websites, or\\ndatabases\\nIndexes Efficient storage of data Creating a searchable index of\\ndocuments\\nData Loaders in LangChain?\\nData loaders are the foundation of any LangChain application. They allow you \\nto import various types of data into your LLM-powered applications. Think of \\nthem as adapters between your files and the AI model.\\nAnatomy of a Data Loader\\nEach LangChain data loader typically follows this pattern:\\n\\ue072\\ue094 Initialization: Set up the loader with parameters like file path\\n\\ue073\\ue094 Loading: Call the l o a d ( )  method to retrieve documents\\n\\ue074\\ue094 Processing: Convert the source into Document objects with text and \\nmetadata\\nCommon Data Loader Properties\\nDocument objects: The standard output format containing:\\np a g e _ c o n t e n t \\ue092 The actual text content\\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n2\"), Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='m e t a d a t a \\ue092 Source information, page numbers, etc.\\nBest Practices for Data Loaders\\nError handling: Always wrap loaders in try/except blocks to handle file \\naccess issues\\nMetadata enrichment: Add source information to help trace data origins\\nChunking strategy: Consider how data will be split after loading\\nData cleaning: Remove irrelevant content like headers/footers when \\npossible\\nPerformance: For large datasets, use streaming or lazy loading techniques\\n1. How to Load PDF Files\\nfrom langchain.document_loaders import PyPDFLoader\\n# Load a PDF file\\nloader \\ue09b PyPDFLoader(\"my_document.pdf\")\\ndocuments = loader.load()\\n# Print the first page content\\nprint(documents[0].page_content[:100])\\nThis code reads a PDF file called \"my_document.pdf\" and prints the first 100 \\ncharacters from the first page.\\n2. How to Load Webpages\\nfrom langchain.document_loaders import WebBaseLoader\\n# Load content from a webpage\\nloader \\ue09b WebBaseLoader(\"https://www.example.com\")\\ndocuments = loader.load()\\n# Print the page content\\nprint(f\"Loaded {len(documents)} document(s)\")\\nprint(documents[0].page_content[:100])\\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n3'), Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='This example gets content from \"example.com\" and prints the beginning of the \\npage content.\\n3. How to Load CSV Data\\nfrom langchain.document_loaders.csv_loader import CSVLoader\\n# Load a CSV file\\nloader \\ue09b CSVLoader(\"my_data.csv\")\\ndocuments = loader.load()\\n# Each row becomes a document\\nprint(f\"Loaded {len(documents)} rows from CSV\")\\nprint(documents[0].page_content)\\nThis loads a CSV file where each row becomes a separate document that can \\nbe processed.\\n4. How to Load Data from Directory\\nfrom langchain.document_loaders import DirectoryLoader\\n# Load all text files in a directory\\nloader \\ue09b DirectoryLoader(\"./my_documents/\", glob=\"**/*.txt\")\\ndocuments = loader.load()\\nprint(f\"Loaded {len(documents)} documents from directory\")\\nThis example loads all text files from a folder called \"my_documents\" and its \\nsubfolders.\\n5. How to Load HTML Data\\nfrom langchain.document_loaders import BSHTMLLoader\\n# Load an HTML file\\nloader \\ue09b BSHTMLLoader(\"webpage.html\")\\ndocuments = loader.load()\\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n4'), Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='print(f\"Loaded HTML content with {len(documents[0].page_content)} char\\nacters\")\\nThis code loads and parses HTML content from a local file.\\n6. How to Load JSON Data\\nfrom langchain.document_loaders import JSONLoader\\nimport json\\n# Define a simple function to extract the content\\ndef extract_content(record):\\n    return record[\"content\"]\\n# Load a JSON file\\nloader \\ue09b JSONLoader(\\n    file_path=\"data.json\",\\n    jq_schema=\\'.[]\\',\\n    content_key=\"content\"\\n)\\ndocuments = loader.load()\\nprint(f\"Loaded {len(documents)} items from JSON\")\\nThis example loads data from a JSON file, extracting content from each item.\\n7. How to Load Markdown Data\\nfrom langchain.document_loaders import UnstructuredMarkdownLoader\\n# Load a markdown file\\nloader \\ue09b UnstructuredMarkdownLoader(\"readme.md\")\\ndocuments = loader.load()\\nprint(f\"Loaded markdown with {len(documents[0].page_content)} characte\\nrs\")\\nThis loads and processes a Markdown file, converting it to plain text content.\\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n5'), Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='8. How to Load Microsoft Office Data\\nfrom langchain.document_loaders import Docx2txtLoader\\nfrom langchain.document_loaders import UnstructuredPowerPointLoader\\n# Load a Word document\\nword_loader \\ue09b Docx2txtLoader(\"document.docx\")\\nword_docs = word_loader.load()\\n# Load a PowerPoint file\\nppt_loader \\ue09b UnstructuredPowerPointLoader(\"presentation.pptx\")\\nppt_docs = ppt_loader.load()\\nprint(f\"Loaded Word doc with {len(word_docs[0].page_content)} character\\ns\")\\nprint(f\"Loaded PowerPoint with {len(ppt_docs)} slides\")\\nThis example shows how to load both Word documents and PowerPoint \\npresentations.\\n9. How to Write a Custom Document Loader\\nfrom langchain.document_loaders.base import BaseLoader\\nfrom langchain.docstore.document import Document\\nclass MyCustomLoader(BaseLoader):\\n    def __init__(self, file_path):\\n        self.file_path = file_path\\n        \\n    def load(self):\\n        # Custom loading logic\\n        with open(self.file_path, \"r\") as f:\\n            text = f.read()\\n        \\n        # Process the text as needed\\n        processed_text = text.upper()  # Just an example - converts to upperc\\nase\\n        \\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n6'), Document(metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-09-28T16:19:18+00:00', 'title': 'Day 1 - LangChain Fundamentals and Data Loader', 'moddate': '2025-09-28T16:19:18+00:00', 'source': 'day1-langchain.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='# Return as a Document object\\n        metadata = {\"source\": self.file_path}\\n        return \\ue083Document(page_content=processed_text, metadata=metadat\\na)]\\n# Use your custom loader\\ncustom_loader \\ue09b MyCustomLoader(\"my_special_format.txt\")\\ndocuments = custom_loader.load()\\nprint(documents[0].page_content[:100])\\nThis example shows how to create your own loader for specialized file formats \\nor data sources.\\nSummary\\nIn this guide, we\\'ve covered:\\nHow to set up LangChain.\\nThe core concepts of LangChain\\nVarious data loader types with code examples\\nHow to create a custom data loader\\nLangChain is a powerful tool for working with AI language models and \\nconnecting them to various data sources. The data loaders we\\'ve explored are \\njust the beginning of what you can do with this framework!\\nDa y 1 \\ue088 LangChain F undament als and Dat a Lo ader\\n7')]\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"day1-langchain.pdf\")\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce751836",
   "metadata": {},
   "source": [
    "Now, here it shows first 200 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289782fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1 - LangChain \n",
      "Fundamentals and Data Loader\n",
      "Introduction to LangChain for Beginners\n",
      "Welcome to this beginner-friendly guide to LangChain! We'll explore the basics, \n",
      "setup, and how to work with dif\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addf056",
   "metadata": {},
   "source": [
    "#### 2. Load Webpages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0204ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355d3b2",
   "metadata": {},
   "source": [
    "To extract any data we add a link of that webpage.\n",
    "- Here I enter a python for beginners link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "528bf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.python.org/about/gettingstarted/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d022d",
   "metadata": {},
   "source": [
    "Now we have to remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cd01131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python For Beginners | Python.org Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Skip to content ▼ Close Python PSF Docs PyPI Jobs Community ▲ The Python Network Donate ≡ Menu Search This Site GO A A Smaller Larger Reset Socialize LinkedIn Mastodon Chat on IRC Twitter About Applications Quotes Getting Started Help Python Brochure Downloads All releases Source code Windows macOS Android Other Platforms License Alternative Implementations Documentation Docs Audio/Visual Talks Beginner's Guide Developer's Guide FAQ Non-English Docs PEP Index Python Books Python Essays Community Diversity Mailing Lists IRC Forums PSF Annual Impact Report Python Conferences Special Interest Groups Python Logo Python Wiki Code of Conduct Community Awards Get Involved Shared Stories Success Stories Arts Business Education Engineering Government Scientific Software Development News Python News PSF\n"
     ]
    }
   ],
   "source": [
    "text = documents[0].page_content.strip()\n",
    "text = \" \".join(text.split()) \n",
    "\n",
    "print(text[:1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c2223",
   "metadata": {},
   "source": [
    "#### 3. Load HTML Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "108e7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14f80502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This HTML file is dummy to understand document loader\n",
      "\n",
      "Linkedin LangChain Series\n",
      "\n",
      "Linkedin LangChain Series\n",
      "\n",
      "Linkedin LangChain Series\n",
      "\n",
      "Linkedin LangChain Series\n",
      "\n",
      "Linkedin LangChain Series\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredHTMLLoader(\"index.html\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b055544",
   "metadata": {},
   "source": [
    "#### 4. Load Markdown Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fccfc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc81e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain\n",
      "\n",
      "Installation Guide\n",
      "\n",
      "conda create -n environment-name python=3.11 conda activate environment-name\n",
      "\n",
      "pip install --upgrade \"langchain>=0.3,<0.4\" pip install --upgrade \"langchain-community>=0.3,<0.4\" pip install --upgrade \"langchain-text-splitters>=0.3,<0.4\" pip install --upgrade \"langchain-core>=0.3,<0.4\" pip install google-generativeai\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredMarkdownLoader(\"README.md\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bb63e",
   "metadata": {},
   "source": [
    "#### Write a Custom Document Loader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd1b62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1d8fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomLoader(BaseLoader):\n",
    "    def load(self):\n",
    "        data = \"I am creating my custom loader. This is my first lecture of LangChain Series.\"\n",
    "        return [Document(page_content=data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b201ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am creating my custom loader. This is my first lecture of LangChain Series.\n"
     ]
    }
   ],
   "source": [
    "loader = MyCustomLoader()\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
